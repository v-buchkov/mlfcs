{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vol_predict.models.mixture.tm_pred2 import (\n",
    "    TM_N_Predictor,\n",
    "    TM_LN_Predictor,\n",
    "    TM_IG_Predictor,\n",
    "    TM_W_Predictor,\n",
    "    TM_HN_W_Predictor,\n",
    "    TM_HN_IG_Predictor\n",
    ")\n",
    "from vol_predict.loss.tm_loss import (\n",
    "    MixtureNormalNLL,\n",
    "    HingeNormalMixtureNLL,\n",
    "    MixtureLogNormalNLL,\n",
    "    MixtureInverseGaussianNLL,\n",
    "    MixtureWeibullNLL,\n",
    "    GenericMixtureNLL,\n",
    "    MixtureHingeNormalWeibullNLL,\n",
    "    MixtureHingeNormalInvGaussianNLL\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Reproducibility                                                            #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # might slow down but ensures reproducibility\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Data utilities                                                             #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def preprocess_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load, clean and standardise the raw CSV exactly as required.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(csv_path)\n",
    "\n",
    "    # --- drop unneeded columns ------------------------------------------------\n",
    "    data.dropna(inplace=True)\n",
    "    cols_to_drop = [c for c in data.columns if c.startswith(\"ask_depth_t\")]\n",
    "    data.drop(columns=cols_to_drop, inplace=True)\n",
    "    cols_to_drop = [c for c in data.columns if c.startswith(\"ret\")]\n",
    "    data.drop(columns=cols_to_drop, inplace=True)\n",
    "    data.drop(columns='datetime', inplace=True)\n",
    "    # --- z-score for *all* features except timestamp / vol --------------------\n",
    "    vol_std = data[\"vol\"].std()\n",
    "\n",
    "    for col in data.columns[1:]:\n",
    "        m, s = data[col].mean(), data[col].std()\n",
    "        data[col] = (data[col] - m) / s\n",
    "\n",
    "    data[\"vol\"] = data[\"vol\"] / vol_std\n",
    "    return data\n",
    "\n",
    "\n",
    "class TMDataset(Dataset):\n",
    "    \"\"\"Dataset producing (past_vol, features_flat, target_vol).\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, ar_order: int = 16):\n",
    "        super().__init__()\n",
    "        self.ar_order = ar_order\n",
    "\n",
    "        self.vol_array = df[\"vol\"].values.astype(np.float32)\n",
    "        self.feat_array = df.iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "        self.valid_idx = range(ar_order, len(df))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.valid_idx)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = self.valid_idx[idx]\n",
    "        past_vol = self.vol_array[i - self.ar_order : i]           # [ar_order]\n",
    "        feats = self.feat_array[i]                                 # [n*lb]\n",
    "        target = self.vol_array[i]\n",
    "\n",
    "        return (torch.tensor(past_vol, dtype=torch.float32),\n",
    "                torch.tensor(feats, dtype=torch.float32),\n",
    "                torch.tensor(target, dtype=torch.float32))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Model / loss factory                                                       #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def model_and_loss(model_type: str,\n",
    "                   ar_order: int,\n",
    "                   n: int,\n",
    "                   lb: int,\n",
    "                   penalty_coef: float = 1.0,\n",
    "                   kl_weight: float = 0,\n",
    "                   delta: float = 0.0,\n",
    "                   l2_coef: float = 0.0,\n",
    "                   eps: float = 1e-12) -> Tuple[nn.Module, nn.Module]:\n",
    "\n",
    "    if model_type == \"normal\":\n",
    "        model = TM_N_Predictor(ar_order, n, lb)\n",
    "        loss = MixtureNormalNLL(eps=eps, l2_coef=l2_coef)\n",
    "    elif model_type == \"hinge\":\n",
    "        model = TM_N_Predictor(ar_order, n, lb)\n",
    "        loss = HingeNormalMixtureNLL(\n",
    "            penalty_coef=penalty_coef, delta=delta,\n",
    "            eps=eps, l2_coef=l2_coef\n",
    "        )\n",
    "    elif model_type == \"lognormal\":\n",
    "        model = TM_LN_Predictor(ar_order, n, lb)\n",
    "        loss = GenericMixtureNLL(crps_weight=0.1,kl_weight = kl_weight, l2_coef=l2_coef)\n",
    "        loss.set_model(model)\n",
    "    elif model_type == \"inverse_gaussian\":\n",
    "        model = TM_IG_Predictor(ar_order, n, lb)\n",
    "        loss = GenericMixtureNLL(crps_weight=0.1,kl_weight = kl_weight, l2_coef=l2_coef)\n",
    "        loss.set_model(model)\n",
    "    elif model_type == \"weibull\":\n",
    "        model = TM_W_Predictor(ar_order, n, lb)\n",
    "        loss = GenericMixtureNLL(crps_weight=0.1, kl_weight = kl_weight, l2_coef=l2_coef)\n",
    "        loss.set_model(model)\n",
    "    elif model_type == \"hinge_weibull\":\n",
    "        model = TM_HN_W_Predictor(ar_order, n, lb)\n",
    "        loss = MixtureHingeNormalWeibullNLL(\n",
    "            penalty_coef=penalty_coef, delta=delta,kl_weight = kl_weight,\n",
    "            eps=eps, l2_coef=l2_coef\n",
    "        )\n",
    "    elif model_type == \"hinge_inverse_gaussian\":\n",
    "        model = TM_HN_IG_Predictor(ar_order, n, lb)\n",
    "        loss = MixtureHingeNormalInvGaussianNLL(\n",
    "            penalty_coef=penalty_coef, delta=delta,kl_weight = kl_weight,\n",
    "            eps=eps, l2_coef=l2_coef\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type '{model_type}'\")\n",
    "\n",
    "    if hasattr(loss, \"set_model\"):\n",
    "        loss.set_model(model)\n",
    "    return model, loss\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Core training helpers (one epoch & eval)                                   #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def epoch_step(loader: DataLoader,\n",
    "               model: nn.Module,\n",
    "               loss_fn: nn.Module,\n",
    "               optimiser: torch.optim.Optimizer | None,\n",
    "               device: torch.device) -> float:\n",
    "    is_train = optimiser is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    total, n = 0.0, 0\n",
    "    for vol_hist, feats, target in loader:\n",
    "        vol_hist, feats, target = (vol_hist.to(device),\n",
    "                                   feats.to(device),\n",
    "                                   target.to(device))\n",
    "\n",
    "        if is_train:\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "        roll_mean = vol_hist.mean(dim=1)\n",
    "        out = model(vol_hist, feats, roll_mean=roll_mean)\n",
    "        loss = loss_fn(target, out, model)\n",
    "\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimiser.step()\n",
    "\n",
    "        total += loss.item() * target.size(0)\n",
    "        n += target.size(0)\n",
    "\n",
    "    return total / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(loader: DataLoader,\n",
    "                    model: nn.Module,\n",
    "                    loss_fn: nn.Module,\n",
    "                    device: torch.device) -> Dict[str, float]:\n",
    "\n",
    "    model.eval()\n",
    "    tot_sq, tot_abs, n = 0.0, 0.0, 0\n",
    "    for vol_hist, feats, target in loader:\n",
    "        vol_hist, feats, target = (vol_hist.to(device),\n",
    "                                   feats.to(device),\n",
    "                                   target.to(device))\n",
    "        roll_mean = vol_hist.mean(dim=1)\n",
    "        out = model(vol_hist, feats, roll_mean=roll_mean)\n",
    "        pred = out[\"mixture_mean\"]\n",
    "        diff = pred - target\n",
    "        tot_sq += (diff ** 2).sum().item()\n",
    "        tot_abs += diff.abs().sum().item()\n",
    "        n += target.size(0)\n",
    "\n",
    "    return dict(rmse=math.sqrt(tot_sq / n), mae=tot_abs / n, count=n)\n",
    "\n",
    "def winsorize_subset(df_base: pd.DataFrame,\n",
    "                     rows: List[int],\n",
    "                     thresh: float) -> pd.DataFrame:\n",
    "    df_w = df_base.iloc[rows].copy()\n",
    "    df_w = df_w.clip(lower=-thresh, upper=thresh)\n",
    "    return df_w.reset_index(drop=True)\n",
    "    \n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Single-split training (original pipeline)                                  #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def run_single_strategy(df: pd.DataFrame,\n",
    "                        args) -> None:\n",
    "    \"\"\"Keeps the behaviour of the original `run_training` pathway.\"\"\"\n",
    "    device = torch.device(args.device)\n",
    "    dataset = TMDataset(df, ar_order=args.ar_order)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_ds = Subset(dataset, range(train_size))\n",
    "    val_ds = Subset(dataset, range(train_size, train_size + val_size))\n",
    "\n",
    "    n_features = df.shape[1] - 2\n",
    "    if n_features % args.lb != 0:\n",
    "        raise ValueError(\"lb must divide total feature dimension.\")\n",
    "    n_feat = n_features // args.lb\n",
    "\n",
    "    model, loss_fn = model_and_loss(\n",
    "        args.model_type, args.ar_order, n_feat, args.lb,\n",
    "        args.penalty_coef, args.delta, args.l2_coef, args.eps\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5, factor=0.5)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=args.batch, shuffle=False)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        tr = epoch_step(train_loader, model, loss_fn, opt, device)\n",
    "        vl = epoch_step(val_loader, model, loss_fn, None, device)\n",
    "        sched.step(vl)\n",
    "        if vl < best_val:\n",
    "            best_val = vl\n",
    "            best_state = model.state_dict()\n",
    "        print(f\"[single] epoch {epoch:02d}/{args.epochs}  \"\n",
    "              f\"train={tr:.4f}  val={vl:.4f}\")\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    metrics = compute_metrics(val_loader, model, loss_fn, device)\n",
    "    print(f\"[single] final  RMSE={metrics['rmse']:.4f}  \"\n",
    "          f\"MAE={metrics['mae']:.4f}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Rolling / Incremental strategies                                           #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def split_intervals(total_len: int, interval: int) -> List[Tuple[int, int]]:\n",
    "    return [(i, min(i + interval, total_len))\n",
    "            for i in range(0, total_len, interval)]\n",
    "\n",
    "\n",
    "def run_rolling_incremental(df: pd.DataFrame,\n",
    "                            args,\n",
    "                            strategy: str) -> None:\n",
    "    \"\"\"\n",
    "    Either a *rolling* or *incremental* evaluation loop.\n",
    "\n",
    "    rolling:\n",
    "        – test interval has `interval_hours` points\n",
    "        – training uses `back_intervals` previous intervals\n",
    "    incremental:\n",
    "        – test interval as above\n",
    "        – training window grows with each iteration\n",
    "    \"\"\"\n",
    "    device = torch.device(args.device)\n",
    "    dataset = TMDataset(df, ar_order=args.ar_order)\n",
    "    full_len = len(dataset)\n",
    "    intervals = split_intervals(full_len, args.interval_hours)\n",
    "\n",
    "    n_features = df.shape[1] - 1\n",
    "    if n_features % args.lb != 0:\n",
    "        raise ValueError(\"lb must divide total feature dimension.\")\n",
    "    n_feat = n_features // args.lb\n",
    "\n",
    "    history: List[Dict[str, float]] = []\n",
    "    accumulated_train: List[int] = []\n",
    "\n",
    "    for idx, (beg, end) in enumerate(intervals):\n",
    "        if strategy == \"rolling\":\n",
    "            train_beg = max(0, beg - args.back_intervals * args.interval_hours)\n",
    "            train_idx = list(range(train_beg, beg))\n",
    "        else:  # incremental\n",
    "            if idx == 0:\n",
    "                train_idx = list(range(0, beg))\n",
    "            else:\n",
    "                accumulated_train.extend(range(intervals[idx - 1][0],\n",
    "                                               intervals[idx - 1][1]))\n",
    "                train_idx = accumulated_train.copy()\n",
    "\n",
    "        if not train_idx:\n",
    "            # first interval for incremental – skip evaluation\n",
    "            accumulated_train.extend(range(beg, end))\n",
    "            continue\n",
    "\n",
    "        train_ds = Subset(dataset, train_idx)\n",
    "        test_ds = Subset(dataset, range(beg, end))\n",
    "        tr_loader = DataLoader(train_ds, batch_size=args.batch, shuffle=True)\n",
    "        te_loader = DataLoader(test_ds, batch_size=args.batch, shuffle=False)\n",
    "\n",
    "        model, loss_fn = model_and_loss(\n",
    "            args.model_type, args.ar_order, n_feat, args.lb,\n",
    "            args.penalty_coef, args.delta, args.l2_coef, args.eps\n",
    "        )\n",
    "        model.to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "        # --- quick training (few epochs) --------------------------------------\n",
    "        best = float(\"inf\")\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            tr_loss = epoch_step(tr_loader, model, loss_fn, opt, device)\n",
    "            if tr_loss < best:\n",
    "                best = tr_loss\n",
    "                best_state = model.state_dict()\n",
    "\n",
    "        model.load_state_dict(best_state)\n",
    "        metrics = compute_metrics(te_loader, model, loss_fn, device)\n",
    "        print(f\"[{strategy}] interval {idx:02d}  \"\n",
    "              f\"RMSE={metrics['rmse']:.4f}  MAE={metrics['mae']:.4f}\")\n",
    "        history.append(dict(interval=idx, **metrics))\n",
    "\n",
    "        if strategy == \"incremental\":\n",
    "            accumulated_train.extend(range(beg, end))\n",
    "\n",
    "    print(f\"[{strategy}] done – evaluated {len(history)} intervals.\")\n",
    "\n",
    "# --- poprawiona funkcja quick_plot – zgodna z wymiarem gate_weights ---------\n",
    "def quick_plot_separate(loader: DataLoader,\n",
    "                        mdl: nn.Module,\n",
    "                        dev: torch.device,\n",
    "                        title: str,\n",
    "                        train_intervals,\n",
    "                        test_interval: int,\n",
    "                        points: int = 100):\n",
    "    \"\"\"\n",
    "    Rysuje dwa **oddzielne** wykresy:\n",
    "        • fig_pred  – True vs Predicted volatility\n",
    "        • fig_gate  – Gate weights (stacked-area), jeżeli model je zwraca\n",
    "\n",
    "    Zwraca krotkę (fig_pred, fig_gate).  `fig_gate` == None,\n",
    "    gdy model nie zwraca 'gate_weights'.\n",
    "    \"\"\"\n",
    "    preds, trues, gates = [], [], []\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        for vhist, feats, tgt in loader:\n",
    "            vhist, feats = vhist.to(dev), feats.to(dev)\n",
    "            roll_mean = vhist.mean(dim=1)\n",
    "            out = mdl(vhist, feats, roll_mean=roll_mean)\n",
    "            preds.append(out[\"mixture_mean\"].cpu().numpy())\n",
    "            trues.append(tgt.cpu().numpy())\n",
    "            if \"gate_weights\" in out:\n",
    "                gates.append(out[\"gate_weights\"].cpu().numpy())\n",
    "\n",
    "    preds  = np.concatenate(preds)[:points]\n",
    "    trues  = np.concatenate(trues)[:points]\n",
    "    gates  = np.concatenate(gates)[:points] if gates else None\n",
    "    if gates is not None and gates.ndim > 1:\n",
    "        gates = gates[:, 0]          # redukcja do 1-D\n",
    "\n",
    "    # ---------- (1) True vs Pred ------------------------------------\n",
    "    fig_pred, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "    ax.plot(trues, label=\"True\", linewidth=2)\n",
    "    ax.plot(preds, label=\"Predicted\", linewidth=2)\n",
    "    ax.set_ylabel(\"Std-scaled volatility\")\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    ax.set_title(f\"{title}\\n[train: {train_intervals}] → [test: {test_interval}]\")\n",
    "    ax.grid(True)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- (2) Gate weights ------------------------------------\n",
    "    fig_gate = None\n",
    "    if gates is not None:\n",
    "        fig_gate, axg = plt.subplots(1, 1, figsize=(12, 2.5))\n",
    "        x = np.arange(len(gates))\n",
    "        axg.fill_between(x, 0, gates, color=\"#d9d96f\")   # volatility (żółte)\n",
    "        axg.fill_between(x, gates, 1.0, color=\"#7f7f7f\") # order book (szare)\n",
    "        axg.set_ylim(0, 1)\n",
    "        axg.set_ylabel(\"gate g(t)\")\n",
    "        axg.set_xlabel(\"Time step\")\n",
    "        axg.grid(True)\n",
    "        axg.legend([\"Volatility\", \"Order book\"],\n",
    "                   loc=\"upper right\", framealpha=0.9)\n",
    "        plt.show()\n",
    "\n",
    "    return fig_pred, fig_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "/opt/anaconda3/envs/volatilityenv/lib/python3.10/site-packages/torch/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # might slow down but ensures reproducibility\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def results(csv_path = \"../../data/spx/btc-2/data_df.csv\",\n",
    "strategy = \"rolling\" ,\n",
    "model_type = \"hinge\",\n",
    "ar_order = 32,\n",
    "lb = 120,\n",
    "lr = 0.0001,\n",
    "batch = 32,\n",
    "epochs = 70,\n",
    "penalty_coef = 1.0,\n",
    "delta = 0.0,\n",
    "l2_coef = 0.001,\n",
    "kl_weight = 0.05,\n",
    "eps = 1e-12,\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "interval_hours = 720,\n",
    "back_intervals = 2,\n",
    "return_history = False,\n",
    "do_plot = False,\n",
    "do_print = False):\n",
    "    df = preprocess_csv(csv_path)\n",
    "    n_features = df.shape[1] - 1\n",
    "    assert n_features % lb == 0, \"lb must divide total feature dimension.\"\n",
    "    n_feat = n_features // lb\n",
    "\n",
    "    dataset = TMDataset(df, ar_order=ar_order)\n",
    "    full_len = len(dataset)\n",
    "\n",
    "    if strategy == \"single\":\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        train_indices = list(range(train_size))\n",
    "        outlier_thresh = 500\n",
    "        train_df_win = winsorize_subset(df, train_indices, outlier_thresh)\n",
    "        train_ds = TMDataset(train_df_win, ar_order=ar_order)\n",
    "        #train_ds = Subset(dataset, range(train_size))\n",
    "        val_ds = Subset(dataset, range(train_size, len(dataset)))\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=batch, shuffle=False)\n",
    "\n",
    "        model, loss_fn = model_and_loss(model_type, ar_order, n_feat, lb,\n",
    "                                        penalty_coef, delta, l2_coef, eps)\n",
    "        model.to(device)\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5, factor=0.5)\n",
    "\n",
    "        best_val = float(\"inf\")\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            tr = epoch_step(train_loader, model, loss_fn, opt, device)\n",
    "            vl = epoch_step(val_loader, model, loss_fn, None, device)\n",
    "            sched.step(vl)\n",
    "            if vl < best_val:\n",
    "                best_val = vl\n",
    "                best_state = model.state_dict()\n",
    "            print(f\"[single] epoch {epoch:02d}/{epochs}  train={tr:.4f}  val={vl:.4f}\")\n",
    "\n",
    "        model.load_state_dict(best_state)\n",
    "        metrics = compute_metrics(val_loader, model, loss_fn, device)\n",
    "        print(f\"[single] final RMSE={metrics['rmse']:.4f}  MAE={metrics['mae']:.4f}\")\n",
    "        if do_plot:\n",
    "            quick_plot_separate(val_loader, model, device)\n",
    "\n",
    "    else:\n",
    "        intervals = split_intervals(full_len, interval_hours)\n",
    "        history: List[Dict[str, float]] = []\n",
    "        accumulated_train: List[int] = []\n",
    "\n",
    "        for idx, (beg, end) in enumerate(intervals):\n",
    "            if strategy == \"rolling\":\n",
    "                train_beg = max(0, beg - back_intervals * interval_hours)\n",
    "                train_idx = list(range(train_beg, beg))\n",
    "            else:  # incremental\n",
    "                if idx == 0:\n",
    "                    train_idx = list(range(0, beg))\n",
    "                else:\n",
    "                    accumulated_train.extend(range(intervals[idx - 1][0], intervals[idx - 1][1]))\n",
    "                    train_idx = accumulated_train.copy()\n",
    "\n",
    "            if not train_idx:\n",
    "                accumulated_train.extend(range(beg, end))\n",
    "                continue\n",
    "\n",
    "            outlier_thresh = 500\n",
    "            train_df_win = winsorize_subset(df, train_idx, outlier_thresh)\n",
    "            train_ds = TMDataset(train_df_win, ar_order=ar_order)\n",
    "            test_ds = Subset(dataset, range(beg, end))\n",
    "            tr_loader = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "            te_loader = DataLoader(test_ds, batch_size=batch, shuffle=False)\n",
    "\n",
    "            model, loss_fn = model_and_loss(model_type, ar_order, n_feat, lb, kl_weight,\n",
    "                                            penalty_coef, delta, l2_coef, eps)\n",
    "            model.to(device)\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            best = float(\"inf\")\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                tr_loss = epoch_step(tr_loader, model, loss_fn, opt, device)\n",
    "                if tr_loss < best:\n",
    "                    best = tr_loss\n",
    "                    best_state = model.state_dict()\n",
    "\n",
    "            model.load_state_dict(best_state)\n",
    "            metrics = compute_metrics(te_loader, model, loss_fn, device)\n",
    "            if do_print:\n",
    "                print(f\"[{strategy}] interval {idx:02d}  RMSE={metrics['rmse']:.4f}  MAE={metrics['mae']:.4f}\")\n",
    "            history.append(dict(interval=idx, **metrics))\n",
    "            if do_plot:\n",
    "                quick_plot_separate(te_loader, model, device,\n",
    "                    title=f\"{strategy.capitalize()} – interval {idx:02d}\",\n",
    "                    train_intervals=[train_beg // interval_hours + i\n",
    "                                     for i in range((beg - train_beg) // interval_hours)]\n",
    "                                   if strategy == \"rolling\"\n",
    "                                   else list(range(0, idx)),\n",
    "                    test_interval=idx)\n",
    "\n",
    "\n",
    "            if strategy == \"incremental\":\n",
    "                accumulated_train.extend(range(beg, end))\n",
    "\n",
    "        print(f\"[{strategy}] for model {model_type} finished {len(history)} intervals.\")\n",
    "        if return_history:\n",
    "            return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training model: NORMAL ===\n",
      "\n",
      "[rolling] for model normal finished 3 intervals.\n",
      "\n",
      "=== Training model: HINGE ===\n",
      "\n",
      "[rolling] for model hinge finished 3 intervals.\n",
      "\n",
      "=== Training model: LOGNORMAL ===\n",
      "\n",
      "[rolling] for model lognormal finished 3 intervals.\n",
      "\n",
      "=== Training model: INVERSE_GAUSSIAN ===\n",
      "\n",
      "[rolling] for model inverse_gaussian finished 3 intervals.\n",
      "\n",
      "=== Training model: WEIBULL ===\n",
      "\n",
      "[rolling] for model weibull finished 3 intervals.\n",
      "\n",
      "=== Training model: HINGE_WEIBULL ===\n",
      "\n",
      "[rolling] for model hinge_weibull finished 3 intervals.\n",
      "\n",
      "=== Training model: HINGE_INVERSE_GAUSSIAN ===\n",
      "\n",
      "[rolling] for model hinge_inverse_gaussian finished 3 intervals.\n"
     ]
    }
   ],
   "source": [
    "model_list = [\"normal\", \"hinge\", \"lognormal\", \"inverse_gaussian\", \"weibull\",\"hinge_weibull\",\"hinge_inverse_gaussian\"]\n",
    "\n",
    "all_histories = {}\n",
    "all_preds = {}\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(f\"\\n=== Training model: {model_name.upper()} ===\\n\")\n",
    "    history = results(csv_path=\"../../data/spx/btc-2/data_df.csv\",\n",
    "                      strategy=\"rolling\",\n",
    "                      model_type=model_name,\n",
    "                      ar_order=64,\n",
    "                      lb=120,\n",
    "                      lr=0.001,\n",
    "                      batch=32,\n",
    "                      epochs=70,\n",
    "                      penalty_coef=1.0,\n",
    "                      delta=0.0,\n",
    "                      kl_weight=0.05,\n",
    "                      l2_coef=0.01,\n",
    "                      eps=1e-8,\n",
    "                      device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                      interval_hours=720,\n",
    "                      back_intervals=2,\n",
    "                      return_history=True,\n",
    "                      do_plot=False)\n",
    "    all_histories[model_name] = history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">MAE</th>\n",
       "      <th colspan=\"3\" halign=\"left\">RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interval</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>normal</th>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hinge</th>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lognormal</th>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inverse_gaussian</th>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weibull</th>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hinge_weibull</th>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hinge_inverse_gaussian</th>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>hinge_weibull</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>hinge_weibull</td>\n",
       "      <td>normal</td>\n",
       "      <td>weibull</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  MAE                               RMSE  \\\n",
       "interval                            1         2         3              1   \n",
       "normal                       0.000112  0.000087   0.00005       0.000273   \n",
       "hinge                        0.000108  0.000088  0.000062       0.000277   \n",
       "lognormal                    0.000305  0.000313  0.000263       0.000428   \n",
       "inverse_gaussian             0.000156   0.00015  0.000137       0.000293   \n",
       "weibull                      0.000125  0.000151  0.000055       0.000535   \n",
       "hinge_weibull                0.000105    0.0001  0.000057       0.000272   \n",
       "hinge_inverse_gaussian       0.000123  0.000127   0.00008       0.000283   \n",
       "best                    hinge_weibull    normal    normal  hinge_weibull   \n",
       "\n",
       "                                            \n",
       "interval                       2         3  \n",
       "normal                  0.000257  0.000099  \n",
       "hinge                   0.000262  0.000102  \n",
       "lognormal               0.000384  0.000274  \n",
       "inverse_gaussian        0.000281  0.000156  \n",
       "weibull                 0.000296  0.000098  \n",
       "hinge_weibull           0.000265  0.000103  \n",
       "hinge_inverse_gaussian  0.000288  0.000116  \n",
       "best                      normal   weibull  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "vol_std = pd.read_csv(\"../../data/spx/btc-2/data_df.csv\")[\"vol\"].std()\n",
    "rmse_table = {}\n",
    "mae_table = {}\n",
    "\n",
    "for model_name, hist in all_histories.items():\n",
    "    df = pd.DataFrame(hist).set_index('interval')\n",
    "    rmse_table[model_name] = df['rmse']\n",
    "    mae_table[model_name] = df['mae']\n",
    "\n",
    "rmse_df = pd.DataFrame(rmse_table).T \n",
    "mae_df = pd.DataFrame(mae_table).T\n",
    "\n",
    "rmse_df = rmse_df.reindex(sorted(rmse_df.columns), axis=1)\n",
    "mae_df = mae_df.reindex(sorted(mae_df.columns), axis=1)\n",
    "\n",
    "rmse_best = rmse_df.idxmin(axis=0)\n",
    "mae_best = mae_df.idxmin(axis=0)\n",
    "\n",
    "rmse_df.loc[\"best\"] = rmse_best\n",
    "mae_df.loc[\"best\"] = mae_best\n",
    "\n",
    "rmse_df_unscaled = rmse_df.drop(index=\"best\").astype(float) * vol_std\n",
    "mae_df_unscaled = mae_df.drop(index=\"best\").astype(float) * vol_std\n",
    "\n",
    "rmse_df_unscaled.loc[\"best\"] = rmse_df_unscaled.idxmin(axis=0)\n",
    "mae_df_unscaled.loc[\"best\"] = mae_df_unscaled.idxmin(axis=0)\n",
    "\n",
    "combined = pd.concat(\n",
    "    [rmse_df_unscaled, mae_df_unscaled],\n",
    "    keys=['RMSE', 'MAE'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "combined = combined.reorder_levels([0,1], axis=1).sort_index(axis=1, level=0)\n",
    "\n",
    "display(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.1,0.01,0.001,0.0001]\n",
    "if False:\n",
    "    for lr in lr_list:\n",
    "        model_list = [\"normal\", \"hinge\", \"lognormal\", \"inverse_gaussian\", \"weibull\",\"hinge_weibull\",\"hinge_inverse_gaussian\"]\n",
    "\n",
    "        all_histories = {}\n",
    "        all_preds = {}\n",
    "\n",
    "        for model_name in model_list:\n",
    "            print(f\"\\n=== Training model: {model_name.upper()} ===\\n\")\n",
    "            history = results(csv_path=\"../../data/spx/btc-2/data_df.csv\",\n",
    "                            strategy=\"rolling\",\n",
    "                            model_type=model_name,\n",
    "                            ar_order=64,\n",
    "                            lb=120,\n",
    "                            lr=0.001,\n",
    "                            batch=32,\n",
    "                            epochs=70,\n",
    "                            penalty_coef=1.0,\n",
    "                            delta=0.0,\n",
    "                            kl_weight=0.05,\n",
    "                            l2_coef=0.01,\n",
    "                            eps=1e-8,\n",
    "                            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                            interval_hours=720,\n",
    "                            back_intervals=2,\n",
    "                            return_history=True,\n",
    "                            do_plot=False)\n",
    "            all_histories[model_name] = history\n",
    "\n",
    "        vol_std = pd.read_csv(\"../../data/spx/btc-2/data_df.csv\")[\"vol\"].std()\n",
    "        rmse_table = {}\n",
    "        mae_table = {}\n",
    "\n",
    "        for model_name, hist in all_histories.items():\n",
    "            df = pd.DataFrame(hist).set_index('interval')\n",
    "            rmse_table[model_name] = df['rmse']\n",
    "            mae_table[model_name] = df['mae']\n",
    "\n",
    "        rmse_df = pd.DataFrame(rmse_table).T \n",
    "        mae_df = pd.DataFrame(mae_table).T\n",
    "\n",
    "        rmse_df = rmse_df.reindex(sorted(rmse_df.columns), axis=1)\n",
    "        mae_df = mae_df.reindex(sorted(mae_df.columns), axis=1)\n",
    "\n",
    "        rmse_best = rmse_df.idxmin(axis=0)\n",
    "        mae_best = mae_df.idxmin(axis=0)\n",
    "\n",
    "        rmse_df.loc[\"best\"] = rmse_best\n",
    "        mae_df.loc[\"best\"] = mae_best\n",
    "\n",
    "        rmse_df_unscaled = rmse_df.drop(index=\"best\").astype(float) * vol_std\n",
    "        mae_df_unscaled = mae_df.drop(index=\"best\").astype(float) * vol_std\n",
    "\n",
    "        rmse_df_unscaled.loc[\"best\"] = rmse_df_unscaled.idxmin(axis=0)\n",
    "        mae_df_unscaled.loc[\"best\"] = mae_df_unscaled.idxmin(axis=0)\n",
    "\n",
    "        combined = pd.concat(\n",
    "            [rmse_df_unscaled, mae_df_unscaled],\n",
    "            keys=['RMSE', 'MAE'],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        combined = combined.reorder_levels([0,1], axis=1).sort_index(axis=1, level=0)\n",
    "        print('The current learning rate is: ',lr)\n",
    "        display(combined)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5db95d9d4a0def798e6b937e5c9712bab7386325b7f2ac07799ca798fbdb4ea1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.16 64-bit ('volatilityenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
