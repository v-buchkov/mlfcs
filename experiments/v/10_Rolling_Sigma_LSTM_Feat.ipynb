{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1510887505e2402",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:03.601759Z",
     "start_time": "2025-05-19T10:43:03.147553Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:06.097274Z",
     "start_time": "2025-05-19T10:43:03.717702Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from config.model_config import ModelConfig\n",
    "from config.experiment_config import ExperimentConfig, AvailableDatasets\n",
    "from vol_predict.features.preprocessor import OneToOnePreprocessor\n",
    "from vol_predict.loss.loss import Loss\n",
    "\n",
    "from vol_predict.models.dl.sigma_lstm_feat_predictor import (\n",
    "    SigmaLSTMFeatPredictor as Model,\n",
    ")\n",
    "from vol_predict.models.baselines.naive_predictor import NaivePredictor as Baseline\n",
    "\n",
    "from run import run_backtest, initialize_sequential_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4448021827516c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:06.616670Z",
     "start_time": "2025-05-19T10:43:06.114460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available data from 2018-06-04 23:00:00 to 2018-09-30 21:00:00\n"
     ]
    }
   ],
   "source": [
    "config = ExperimentConfig()\n",
    "config.DATASET = AvailableDatasets.BITCOIN\n",
    "\n",
    "model_params = ModelConfig()\n",
    "baseline_params = ModelConfig()\n",
    "\n",
    "model_params.n_features = 1200\n",
    "model_params.n_unique_features = 10\n",
    "\n",
    "# Handles the features\n",
    "feature_processor = OneToOnePreprocessor()\n",
    "\n",
    "runner = initialize_sequential_runner(\n",
    "    model_config=model_params,\n",
    "    preprocessor=feature_processor,\n",
    "    experiment_config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81afe07a0a2b6ce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:20.644450Z",
     "start_time": "2025-05-19T10:43:06.640247Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1/20:  32%|███▏      | 12/38 [00:12<00:26,  1.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m runner.model_config.loss = Loss.NLL\n\u001b[32m      8\u001b[39m runner.model_config.dropout = \u001b[32m0.20\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m result = \u001b[43mrun_backtest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbaseline_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/run.py:68\u001b[39m, in \u001b[36mrun_backtest\u001b[39m\u001b[34m(model_cls, baseline_cls, runner, experiment_config)\u001b[39m\n\u001b[32m     65\u001b[39m model = model_cls(**runner.model_config.dict())\n\u001b[32m     66\u001b[39m baseline = baseline_cls(**runner.model_config.dict())\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m run_result = \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m run_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/vol_predict/sequential_runner.py:198\u001b[39m, in \u001b[36mSequentialRunner.__call__\u001b[39m\u001b[34m(self, model, baseline, n_epochs)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    193\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    194\u001b[39m     model: AbstractPredictor,\n\u001b[32m    195\u001b[39m     baseline: AbstractPredictor,\n\u001b[32m    196\u001b[39m     n_epochs: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    197\u001b[39m ) -> pd.DataFrame:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/vol_predict/sequential_runner.py:155\u001b[39m, in \u001b[36mSequentialRunner.run\u001b[39m\u001b[34m(self, model, baseline, n_epochs)\u001b[39m\n\u001b[32m    146\u001b[39m     baseline = baseline.\u001b[34m__class__\u001b[39m(**\u001b[38;5;28mself\u001b[39m.model_config.dict())\n\u001b[32m    148\u001b[39m model_trainer = Trainer(\n\u001b[32m    149\u001b[39m     train_loader=train_loader,\n\u001b[32m    150\u001b[39m     val_loader=train_loader,\n\u001b[32m    151\u001b[39m     model_config=\u001b[38;5;28mself\u001b[39m.model_config,\n\u001b[32m    152\u001b[39m     experiment_config=\u001b[38;5;28mself\u001b[39m.experiment_config,\n\u001b[32m    153\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[43mmodel_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m baseline_trainer = Trainer(\n\u001b[32m    158\u001b[39m     train_loader=train_loader,\n\u001b[32m    159\u001b[39m     val_loader=train_loader,\n\u001b[32m    160\u001b[39m     model_config=\u001b[38;5;28mself\u001b[39m.model_config,\n\u001b[32m    161\u001b[39m     experiment_config=\u001b[38;5;28mself\u001b[39m.experiment_config,\n\u001b[32m    162\u001b[39m )\n\u001b[32m    164\u001b[39m baseline_trainer(baseline, n_epochs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/vol_predict/train/trainer.py:97\u001b[39m, in \u001b[36mTrainer.__call__\u001b[39m\u001b[34m(self, model, n_epochs)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: AbstractPredictor, n_epochs: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/vol_predict/train/trainer.py:113\u001b[39m, in \u001b[36mTrainer.run\u001b[39m\u001b[34m(self, model, n_epochs)\u001b[39m\n\u001b[32m    106\u001b[39m     n_epochs = \u001b[38;5;28mself\u001b[39m.model_config.n_epochs\n\u001b[32m    108\u001b[39m scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n\u001b[32m    109\u001b[39m     optimizer, T_max=n_epochs\n\u001b[32m    110\u001b[39m )\n\u001b[32m    112\u001b[39m \u001b[38;5;28mself\u001b[39m._train_losses, \u001b[38;5;28mself\u001b[39m._val_losses, \u001b[38;5;28mself\u001b[39m._train_preds, \u001b[38;5;28mself\u001b[39m._val_preds = (\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_val_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_logs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_model:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28mself\u001b[39m.save(model, \u001b[38;5;28mself\u001b[39m.experiment_config.PATH_OUTPUT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/vol_predict/train/trainer.py:61\u001b[39m, in \u001b[36mTrainer._train\u001b[39m\u001b[34m(self, model, optimizer, scheduler, train_loader, val_loader, num_epochs, print_logs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     59\u001b[39m     desc_train, desc_val = \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m train_loss, train_pred = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m val_loss, val_pred = validation_epoch(\n\u001b[32m     71\u001b[39m     model,\n\u001b[32m     72\u001b[39m     criterion,\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     n_layers=\u001b[38;5;28mself\u001b[39m.model_config.n_layers,\n\u001b[32m     77\u001b[39m )\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/vol_predict/train/train.py:116\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, optimizer, criterion, loader, tqdm_desc, max_grad_norm, hidden_size, n_layers)\u001b[39m\n\u001b[32m    113\u001b[39m     loss = criterion(true_returns, true_vols, pred_vol)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m loss.requires_grad:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m         optimizer.step()\n\u001b[32m    119\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/.venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/mlfcs/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "runner.model_config.lr = 1e-2\n",
    "runner.model_config.n_epochs = 20\n",
    "runner.model_config.hidden_size = 64\n",
    "runner.model_config.n_layers = 3\n",
    "runner.model_config.batch_size = 16\n",
    "runner.model_config.optimizer = torch.optim.Adam\n",
    "runner.model_config.loss = Loss.NLL\n",
    "runner.model_config.dropout = 0.20\n",
    "\n",
    "result = run_backtest(\n",
    "    model_cls=Model,\n",
    "    baseline_cls=Baseline,\n",
    "    runner=runner,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81acb91339e5878b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:20.662113Z",
     "start_time": "2025-05-19T05:42:10.138616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_loss</th>\n",
       "      <th>baseline_loss</th>\n",
       "      <th>true_returns</th>\n",
       "      <th>true_vols</th>\n",
       "      <th>model_preds</th>\n",
       "      <th>baseline_preds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>-5.664095</td>\n",
       "      <td>-8.235707</td>\n",
       "      <td>[-0.0026883667, -0.004259642, 0.0017928587, 0....</td>\n",
       "      <td>[0.00041156381, 0.0004322233, 0.00021497025, 0...</td>\n",
       "      <td>[0.0061398936, 0.0016324297, 0.015595073, 0.01...</td>\n",
       "      <td>[0.00023757516, 0.00023757516, 0.00023757516, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-05</th>\n",
       "      <td>-5.440987</td>\n",
       "      <td>-8.218187</td>\n",
       "      <td>[-0.0025450767, 0.0020725208, -0.0004954421, -...</td>\n",
       "      <td>[0.00021121168, 0.0001595019, 0.00016921855, 0...</td>\n",
       "      <td>[0.01433832, 0.008739568, 1.0823251e-05, 0.001...</td>\n",
       "      <td>[0.00025369268, 0.00025369268, 0.00025369268, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-10</th>\n",
       "      <td>-6.082431</td>\n",
       "      <td>-8.389498</td>\n",
       "      <td>[-0.0021792948, 0.0005109866, 0.00070048636, -...</td>\n",
       "      <td>[0.00016044531, 0.00012531331, 8.648752e-05, 7...</td>\n",
       "      <td>[0.0006236774, 0.00038768662, 0.0006722097, 0....</td>\n",
       "      <td>[0.00020761654, 0.00020761654, 0.00020761654, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-15</th>\n",
       "      <td>-4.929691</td>\n",
       "      <td>-8.221523</td>\n",
       "      <td>[0.00090926996, -0.00016162315, 0.0028238427, ...</td>\n",
       "      <td>[6.138905e-05, 4.4802924e-05, 8.2382474e-05, 5...</td>\n",
       "      <td>[0.014383028, 0.004077753, 0.017638417, 0.0017...</td>\n",
       "      <td>[0.00018584127, 0.00018584127, 0.00018584127, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-20</th>\n",
       "      <td>-5.443533</td>\n",
       "      <td>-8.404459</td>\n",
       "      <td>[0.0016087972, -0.0024853398, -0.004938002, -0...</td>\n",
       "      <td>[4.2298943e-05, 4.114835e-05, 7.0722686e-05, 0...</td>\n",
       "      <td>[0.004574802, 0.02644255, 0.00062924204, 0.001...</td>\n",
       "      <td>[0.00019202351, 0.00019202351, 0.00019202351, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-25</th>\n",
       "      <td>-4.833037</td>\n",
       "      <td>-8.423176</td>\n",
       "      <td>[-0.008449829, 0.0038263223, 0.004215324, 0.00...</td>\n",
       "      <td>[0.00024071828, 0.00015700524, 9.3138595e-05, ...</td>\n",
       "      <td>[0.0063968464, 0.092749245, 4.2733987e-05, 0.0...</td>\n",
       "      <td>[0.00017652447, 0.00017652447, 0.00017652447, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-30</th>\n",
       "      <td>-3.805878</td>\n",
       "      <td>-8.469709</td>\n",
       "      <td>[-0.002002476, -0.0008079753, -0.001266979, 0....</td>\n",
       "      <td>[2.0188261e-05, 1.9543379e-05, 2.5798885e-05, ...</td>\n",
       "      <td>[0.009633502, 0.0051093465, 6.165126e-05, 0.00...</td>\n",
       "      <td>[0.00014895704, 0.00014895704, 0.00014895704, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-04</th>\n",
       "      <td>3.115977</td>\n",
       "      <td>-8.463673</td>\n",
       "      <td>[-0.0003308475, -0.0018424237, 0.0016508304, -...</td>\n",
       "      <td>[0.000100981226, 4.602776e-05, 4.093159e-05, 2...</td>\n",
       "      <td>[0.00024363367, 2.500133e-05, 6.5879435e-06, 0...</td>\n",
       "      <td>[0.00014504998, 0.00014504998, 0.00014504998, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-09</th>\n",
       "      <td>-4.291669</td>\n",
       "      <td>-8.301634</td>\n",
       "      <td>[-0.0027170812, 0.0020420607, 0.0029039332, 0....</td>\n",
       "      <td>[3.1182026e-05, 4.331007e-05, 4.167312e-05, 8....</td>\n",
       "      <td>[0.008976123, 0.0022160334, 0.0009541738, 1.25...</td>\n",
       "      <td>[0.00014232029, 0.00014232029, 0.00014232029, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-14</th>\n",
       "      <td>-5.021458</td>\n",
       "      <td>-8.509558</td>\n",
       "      <td>[-0.015192035, 0.008707496, 0.0012189541, -0.0...</td>\n",
       "      <td>[0.0005138371, 0.00011423257, 7.7952835e-05, 3...</td>\n",
       "      <td>[0.009924774, 8.53746e-05, 7.040214e-05, 0.040...</td>\n",
       "      <td>[0.00013563369, 0.00013563369, 0.00013563369, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-19</th>\n",
       "      <td>-5.184609</td>\n",
       "      <td>-8.497404</td>\n",
       "      <td>[0.00284836, -0.005842211, 0.0054638404, 0.001...</td>\n",
       "      <td>[3.1251417e-05, 7.9688376e-05, 3.7512837e-05, ...</td>\n",
       "      <td>[0.001077064, 0.004422056, 2.0473012e-06, 3.20...</td>\n",
       "      <td>[0.00013420901, 0.00013420901, 0.00013420901, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-24</th>\n",
       "      <td>-5.106636</td>\n",
       "      <td>-8.759987</td>\n",
       "      <td>[0.00012925602, -0.0008466228, 0.0011789188, 0...</td>\n",
       "      <td>[4.755799e-05, 2.8825178e-05, 2.4602998e-05, 2...</td>\n",
       "      <td>[7.200717e-06, 0.0014770114, 0.038060695, 0.02...</td>\n",
       "      <td>[0.00013844245, 0.00013844245, 0.00013844245, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-29</th>\n",
       "      <td>-5.212703</td>\n",
       "      <td>-8.800233</td>\n",
       "      <td>[-0.0041914913, 0.0023179988, -0.00036773554, ...</td>\n",
       "      <td>[5.033898e-05, 3.5160283e-05, 1.4736159e-05, 5...</td>\n",
       "      <td>[5.601142e-06, 0.012688426, 0.02906621, 0.0077...</td>\n",
       "      <td>[0.00013175738, 0.00013175738, 0.00013175738, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-03</th>\n",
       "      <td>-5.624711</td>\n",
       "      <td>-8.470111</td>\n",
       "      <td>[0.0010120253, 0.0013711395, -0.0007382839, -0...</td>\n",
       "      <td>[1.4816889e-05, 1.0349518e-05, 4.4979497e-05, ...</td>\n",
       "      <td>[0.014489171, 0.00089589297, 0.0020523237, 0.0...</td>\n",
       "      <td>[0.00012849354, 0.00012849354, 0.00012849354, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.771120</td>\n",
       "      <td>[0.002658868, -0.002891154, 0.00038503538, 0.0...</td>\n",
       "      <td>[2.1529624e-05, 3.2941807e-05, 3.0699048e-05, ...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>[0.0001240236, 0.0001240236, 0.0001240236, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-13</th>\n",
       "      <td>-5.380781</td>\n",
       "      <td>-8.954327</td>\n",
       "      <td>[0.0001642307, -0.0015417017, 0.0018184867, -0...</td>\n",
       "      <td>[3.830286e-05, 1.2657858e-05, 1.3487654e-05, 3...</td>\n",
       "      <td>[0.0031812282, 0.0005272366, 0.007661434, 0.00...</td>\n",
       "      <td>[0.0001127071, 0.0001127071, 0.0001127071, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-18</th>\n",
       "      <td>-6.066285</td>\n",
       "      <td>-8.888277</td>\n",
       "      <td>[-0.00040011652, -0.00105868, 0.0029426306, 0....</td>\n",
       "      <td>[1.7583305e-05, 1.608473e-05, 3.687474e-05, 2....</td>\n",
       "      <td>[0.026947998, 0.032824054, 0.0019251188, 0.000...</td>\n",
       "      <td>[9.389723e-05, 9.389723e-05, 9.389723e-05, 9.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-23</th>\n",
       "      <td>-5.051067</td>\n",
       "      <td>-9.077349</td>\n",
       "      <td>[0.0010780431, 0.0013246548, 0.0028881382, -0....</td>\n",
       "      <td>[2.0660073e-06, 6.2520367e-06, 8.583689e-06, 8...</td>\n",
       "      <td>[0.06477176, 0.011291286, 0.0018617433, 0.0010...</td>\n",
       "      <td>[9.690705e-05, 9.690705e-05, 9.690705e-05, 9.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-28</th>\n",
       "      <td>-4.580041</td>\n",
       "      <td>-9.097578</td>\n",
       "      <td>[-0.0022302242, 0.00031208308, 0.0011399924, 0...</td>\n",
       "      <td>[4.492918e-05, 1.22871825e-05, 6.9373027e-06, ...</td>\n",
       "      <td>[0.009097284, 0.0021422394, 0.02447045, 0.0034...</td>\n",
       "      <td>[9.148046e-05, 9.148046e-05, 9.148046e-05, 9.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model_loss  baseline_loss  \\\n",
       "datetime                                \n",
       "2018-06-30   -5.664095      -8.235707   \n",
       "2018-07-05   -5.440987      -8.218187   \n",
       "2018-07-10   -6.082431      -8.389498   \n",
       "2018-07-15   -4.929691      -8.221523   \n",
       "2018-07-20   -5.443533      -8.404459   \n",
       "2018-07-25   -4.833037      -8.423176   \n",
       "2018-07-30   -3.805878      -8.469709   \n",
       "2018-08-04    3.115977      -8.463673   \n",
       "2018-08-09   -4.291669      -8.301634   \n",
       "2018-08-14   -5.021458      -8.509558   \n",
       "2018-08-19   -5.184609      -8.497404   \n",
       "2018-08-24   -5.106636      -8.759987   \n",
       "2018-08-29   -5.212703      -8.800233   \n",
       "2018-09-03   -5.624711      -8.470111   \n",
       "2018-09-08         NaN      -8.771120   \n",
       "2018-09-13   -5.380781      -8.954327   \n",
       "2018-09-18   -6.066285      -8.888277   \n",
       "2018-09-23   -5.051067      -9.077349   \n",
       "2018-09-28   -4.580041      -9.097578   \n",
       "\n",
       "                                                 true_returns  \\\n",
       "datetime                                                        \n",
       "2018-06-30  [-0.0026883667, -0.004259642, 0.0017928587, 0....   \n",
       "2018-07-05  [-0.0025450767, 0.0020725208, -0.0004954421, -...   \n",
       "2018-07-10  [-0.0021792948, 0.0005109866, 0.00070048636, -...   \n",
       "2018-07-15  [0.00090926996, -0.00016162315, 0.0028238427, ...   \n",
       "2018-07-20  [0.0016087972, -0.0024853398, -0.004938002, -0...   \n",
       "2018-07-25  [-0.008449829, 0.0038263223, 0.004215324, 0.00...   \n",
       "2018-07-30  [-0.002002476, -0.0008079753, -0.001266979, 0....   \n",
       "2018-08-04  [-0.0003308475, -0.0018424237, 0.0016508304, -...   \n",
       "2018-08-09  [-0.0027170812, 0.0020420607, 0.0029039332, 0....   \n",
       "2018-08-14  [-0.015192035, 0.008707496, 0.0012189541, -0.0...   \n",
       "2018-08-19  [0.00284836, -0.005842211, 0.0054638404, 0.001...   \n",
       "2018-08-24  [0.00012925602, -0.0008466228, 0.0011789188, 0...   \n",
       "2018-08-29  [-0.0041914913, 0.0023179988, -0.00036773554, ...   \n",
       "2018-09-03  [0.0010120253, 0.0013711395, -0.0007382839, -0...   \n",
       "2018-09-08  [0.002658868, -0.002891154, 0.00038503538, 0.0...   \n",
       "2018-09-13  [0.0001642307, -0.0015417017, 0.0018184867, -0...   \n",
       "2018-09-18  [-0.00040011652, -0.00105868, 0.0029426306, 0....   \n",
       "2018-09-23  [0.0010780431, 0.0013246548, 0.0028881382, -0....   \n",
       "2018-09-28  [-0.0022302242, 0.00031208308, 0.0011399924, 0...   \n",
       "\n",
       "                                                    true_vols  \\\n",
       "datetime                                                        \n",
       "2018-06-30  [0.00041156381, 0.0004322233, 0.00021497025, 0...   \n",
       "2018-07-05  [0.00021121168, 0.0001595019, 0.00016921855, 0...   \n",
       "2018-07-10  [0.00016044531, 0.00012531331, 8.648752e-05, 7...   \n",
       "2018-07-15  [6.138905e-05, 4.4802924e-05, 8.2382474e-05, 5...   \n",
       "2018-07-20  [4.2298943e-05, 4.114835e-05, 7.0722686e-05, 0...   \n",
       "2018-07-25  [0.00024071828, 0.00015700524, 9.3138595e-05, ...   \n",
       "2018-07-30  [2.0188261e-05, 1.9543379e-05, 2.5798885e-05, ...   \n",
       "2018-08-04  [0.000100981226, 4.602776e-05, 4.093159e-05, 2...   \n",
       "2018-08-09  [3.1182026e-05, 4.331007e-05, 4.167312e-05, 8....   \n",
       "2018-08-14  [0.0005138371, 0.00011423257, 7.7952835e-05, 3...   \n",
       "2018-08-19  [3.1251417e-05, 7.9688376e-05, 3.7512837e-05, ...   \n",
       "2018-08-24  [4.755799e-05, 2.8825178e-05, 2.4602998e-05, 2...   \n",
       "2018-08-29  [5.033898e-05, 3.5160283e-05, 1.4736159e-05, 5...   \n",
       "2018-09-03  [1.4816889e-05, 1.0349518e-05, 4.4979497e-05, ...   \n",
       "2018-09-08  [2.1529624e-05, 3.2941807e-05, 3.0699048e-05, ...   \n",
       "2018-09-13  [3.830286e-05, 1.2657858e-05, 1.3487654e-05, 3...   \n",
       "2018-09-18  [1.7583305e-05, 1.608473e-05, 3.687474e-05, 2....   \n",
       "2018-09-23  [2.0660073e-06, 6.2520367e-06, 8.583689e-06, 8...   \n",
       "2018-09-28  [4.492918e-05, 1.22871825e-05, 6.9373027e-06, ...   \n",
       "\n",
       "                                                  model_preds  \\\n",
       "datetime                                                        \n",
       "2018-06-30  [0.0061398936, 0.0016324297, 0.015595073, 0.01...   \n",
       "2018-07-05  [0.01433832, 0.008739568, 1.0823251e-05, 0.001...   \n",
       "2018-07-10  [0.0006236774, 0.00038768662, 0.0006722097, 0....   \n",
       "2018-07-15  [0.014383028, 0.004077753, 0.017638417, 0.0017...   \n",
       "2018-07-20  [0.004574802, 0.02644255, 0.00062924204, 0.001...   \n",
       "2018-07-25  [0.0063968464, 0.092749245, 4.2733987e-05, 0.0...   \n",
       "2018-07-30  [0.009633502, 0.0051093465, 6.165126e-05, 0.00...   \n",
       "2018-08-04  [0.00024363367, 2.500133e-05, 6.5879435e-06, 0...   \n",
       "2018-08-09  [0.008976123, 0.0022160334, 0.0009541738, 1.25...   \n",
       "2018-08-14  [0.009924774, 8.53746e-05, 7.040214e-05, 0.040...   \n",
       "2018-08-19  [0.001077064, 0.004422056, 2.0473012e-06, 3.20...   \n",
       "2018-08-24  [7.200717e-06, 0.0014770114, 0.038060695, 0.02...   \n",
       "2018-08-29  [5.601142e-06, 0.012688426, 0.02906621, 0.0077...   \n",
       "2018-09-03  [0.014489171, 0.00089589297, 0.0020523237, 0.0...   \n",
       "2018-09-08  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...   \n",
       "2018-09-13  [0.0031812282, 0.0005272366, 0.007661434, 0.00...   \n",
       "2018-09-18  [0.026947998, 0.032824054, 0.0019251188, 0.000...   \n",
       "2018-09-23  [0.06477176, 0.011291286, 0.0018617433, 0.0010...   \n",
       "2018-09-28  [0.009097284, 0.0021422394, 0.02447045, 0.0034...   \n",
       "\n",
       "                                               baseline_preds  \n",
       "datetime                                                       \n",
       "2018-06-30  [0.00023757516, 0.00023757516, 0.00023757516, ...  \n",
       "2018-07-05  [0.00025369268, 0.00025369268, 0.00025369268, ...  \n",
       "2018-07-10  [0.00020761654, 0.00020761654, 0.00020761654, ...  \n",
       "2018-07-15  [0.00018584127, 0.00018584127, 0.00018584127, ...  \n",
       "2018-07-20  [0.00019202351, 0.00019202351, 0.00019202351, ...  \n",
       "2018-07-25  [0.00017652447, 0.00017652447, 0.00017652447, ...  \n",
       "2018-07-30  [0.00014895704, 0.00014895704, 0.00014895704, ...  \n",
       "2018-08-04  [0.00014504998, 0.00014504998, 0.00014504998, ...  \n",
       "2018-08-09  [0.00014232029, 0.00014232029, 0.00014232029, ...  \n",
       "2018-08-14  [0.00013563369, 0.00013563369, 0.00013563369, ...  \n",
       "2018-08-19  [0.00013420901, 0.00013420901, 0.00013420901, ...  \n",
       "2018-08-24  [0.00013844245, 0.00013844245, 0.00013844245, ...  \n",
       "2018-08-29  [0.00013175738, 0.00013175738, 0.00013175738, ...  \n",
       "2018-09-03  [0.00012849354, 0.00012849354, 0.00012849354, ...  \n",
       "2018-09-08  [0.0001240236, 0.0001240236, 0.0001240236, 0.0...  \n",
       "2018-09-13  [0.0001127071, 0.0001127071, 0.0001127071, 0.0...  \n",
       "2018-09-18  [9.389723e-05, 9.389723e-05, 9.389723e-05, 9.3...  \n",
       "2018-09-23  [9.690705e-05, 9.690705e-05, 9.690705e-05, 9.6...  \n",
       "2018-09-28  [9.148046e-05, 9.148046e-05, 9.148046e-05, 9.1...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4df28114baaad36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:20.665948Z",
     "start_time": "2025-05-19T05:58:46.095411Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "rmse = []\n",
    "for date, row in result.iterrows():\n",
    "    if not np.isnan(row.model_preds).any():\n",
    "        rmse.append(\n",
    "            [\n",
    "                date,\n",
    "                root_mean_squared_error(row.true_vols, row.model_preds),\n",
    "                root_mean_squared_error(row.true_vols, row.baseline_preds),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "rmse = pd.DataFrame(rmse, columns=[\"date\", \"model\", \"baseline\"]).set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "297e27681336c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:20.666421Z",
     "start_time": "2025-05-19T05:58:47.349839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model       0.032062\n",
       "baseline    0.000193\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c128907e5cdbc938",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:20.677746Z",
     "start_time": "2025-05-19T05:58:50.053396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model       0.022340\n",
       "baseline    0.000151\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e3cdd01c2954175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:20.691729Z",
     "start_time": "2025-05-19T05:57:11.238596Z"
    }
   },
   "outputs": [],
   "source": [
    "# result.to_csv(f\"Sigma_Feat_{runner.model_config.n_epochs}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0496b69b8675f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T10:43:20.707294Z",
     "start_time": "2025-05-19T00:26:47.128618Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
